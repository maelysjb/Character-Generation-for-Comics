{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Model: DreamBooth (Google Colab)\n",
    "\n",
    "This notebook goes through the modeling and inference code to implement a DreamBooth Stable Diffusion model using google colab. \n",
    "\n",
    "The main notebook utilized in this project can be found on our git in the folder 4. Fine Tuning Models, labeled: \n",
    "- \"DreamBooth.ipynb\"\n",
    "- \"DreamBooth-Inference.ipynb\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment \n",
    "\n",
    "We changed the settings of our Google Colab in order to speed up the processing operations. \n",
    "\n",
    "- Change the runtime type to T4 GPU\n",
    "    - Ensure that the memory size is at lead 12GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure memory size for the google colab \n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Install Diffusion Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Image Display\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffuser libraries \n",
    "\n",
    "!pip install -qq \"ipywidgets>=7,<8\"\n",
    "!git clone https://github.com/huggingface/diffusers\n",
    "%cd /content/diffusers\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DreamBooth requirements & xFormers Library \n",
    "\n",
    "%cd /content/diffusers/examples/dreambooth\n",
    "!pip install -r requirements.txt\n",
    "!pip install bitsandbytes\n",
    "!pip install transformers gradio ftfy accelerate\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training \n",
    "from diffusers import DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face \n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "if os.path.exists(\"/content/custom_dataset\"):\n",
    "    print(\"Removing existing custom_dataset folder\")\n",
    "    !rm -rf /content/custom_dataset\n",
    "\n",
    "print(\"Creating new custom_dataset folder\")\n",
    "!mkdir /content/custom_dataset\n",
    "!mkdir /content/custom_dataset/class_images\n",
    "!mkdir /content/custom_dataset/instance_images\n",
    "\n",
    "print('Custom Dataset folder is created: /content/custom_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data size function \n",
    "\n",
    "def resize_and_crop_images(folder_path, target_size=512):\n",
    "    \"\"\"\n",
    "    Resize the images in a folder to have a smaller edge of the specified target size and save them to a new location.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing the images.\n",
    "    - target_size (int): Desired size for the smaller edge (default is 512).\n",
    "    \"\"\"\n",
    "    # Define the output folder for resized and cropped images\n",
    "    output_folder = '/kaggle/working/resized_images'\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            # Open the image\n",
    "            image = Image.open(file_path)\n",
    "\n",
    "            # Get the original width and height\n",
    "            width, height = image.size\n",
    "\n",
    "            # Calculate the new size while maintaining the aspect ratio\n",
    "            if width <= height:\n",
    "                new_width = target_size\n",
    "                new_height = int(height * (target_size / width))\n",
    "            else:\n",
    "                new_width = int(width * (target_size / height))\n",
    "                new_height = target_size\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = image.resize((new_width, new_height))\n",
    "\n",
    "            left = (new_width - target_size) // 2\n",
    "            top = (new_height - target_size) // 2\n",
    "            right = (new_width + target_size) // 2\n",
    "            bottom = (new_height + target_size) // 2\n",
    "\n",
    "            # Perform the center crop\n",
    "            cropped_image = resized_image.crop((left, top, right, bottom))\n",
    "            \n",
    "            # Save the cropped image to the output folder\n",
    "            cropped_image.save(os.path.join(output_folder, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting images function \n",
    "\n",
    "def show_images_in_one_row(folder_path, target_size=256):\n",
    "    images = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize((target_size, int(target_size * img.size[1] / img.size[0])))\n",
    "            images.append(img)\n",
    "\n",
    "    # Display images in one row\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(len(images) * 3, 3))\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Images\n",
    "folder_path = '/content/custom_dataset/class_images'\n",
    "if len(os.listdir(folder_path)):\n",
    "  resize_and_crop_images(folder_path)\n",
    "  show_images_in_one_row(folder_path)\n",
    "\n",
    "# Instance Images\n",
    "folder_path = '/content/custom_dataset/instance_images'\n",
    "resize_and_crop_images(folder_path)\n",
    "show_images_in_one_row(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/content/outputs\"):\n",
    "    print(\"Removing existing outputs folder\")\n",
    "    !rm -rf /content/outputs\n",
    "\n",
    "print(\"Creating new outputs folder\")\n",
    "!mkdir /content/outputs\n",
    "\n",
    "print('Output folder is created: /content/outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login into Hugging Face account \n",
    "\n",
    "Replace the name for the Hugging Face token where it states: \"TOKEN_FROM_HF\" to the desired name. This will be your own personal Hugging Gace token in order to save a private model and dataset. \n",
    "\n",
    "Instructions on using Hugging Face can be found here: https://github.com/maelysjb/Comics-GenAI/blob/main/README.md#:~:text=.gitignore-,README,-.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"TOKEN_FROM_HF\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dreambooth Diffusion Model \n",
    "\n",
    "Replace the name for the Hugging Face model id where it states: \"DreamBooth200\" to the Hugging Face new model name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_dreambooth.py --pretrained_model_name_or_path 'runwayml/stable-diffusion-v1-5' \\\n",
    "                            --revision \"fp16\" \\\n",
    "                            --instance_data_dir '/content/custom_dataset/instance_images' \\\n",
    "                            --class_data_dir '/content/custom_dataset/class_images' \\\n",
    "                            --instance_prompt 'An image of UnicornGirl in unicorn onesie.' \\\n",
    "                            --class_prompt 'An image of UnicornGirl in unicorn onesie.' \\\n",
    "                            --with_prior_preservation \\\n",
    "                            --prior_loss_weight 1.0 \\\n",
    "                            --num_class_images 100 \\\n",
    "                            --output_dir '/content/outputs' \\\n",
    "                            --resolution 512 \\\n",
    "                            --train_text_encoder \\\n",
    "                            --train_batch_size 2 \\\n",
    "                            --sample_batch_size 2 \\\n",
    "                            --max_train_steps 2000 \\\n",
    "                            --checkpointing_steps 1900 \\\n",
    "                            --gradient_accumulation_steps 1 \\\n",
    "                            --gradient_checkpointing \\\n",
    "                            --learning_rate 1e-6 \\\n",
    "                            --lr_scheduler 'constant' \\\n",
    "                            --lr_warmup_steps=0 \\\n",
    "                            --use_8bit_adam \\\n",
    "                            --validation_prompt 'An image of UnicornGirl in a unicorn onesie.' \\\n",
    "                            --num_validation_images 4 \\\n",
    "                            --mixed_precision \"fp16\" \\\n",
    "                            --enable_xformers_memory_efficient_attention \\\n",
    "                            --set_grads_to_none \\\n",
    "                            --push_to_hub \\\n",
    "                            --hub_model_id DreamBooth2000 \n",
    "                            #--report_to 'wandb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = '/content/outputs'\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(trained_model_path + '/unet')\n",
    "text_encoder = CLIPTextModel.from_pretrained(trained_model_path + '/text_encoder')\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", unet=unet,\n",
    "    text_encoder=text_encoder, dtype=torch.float16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, num_samples, negative_prompt, guidance_scale,\n",
    "              num_inference_steps, height, width):\n",
    "    images = pipeline(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale\n",
    "    ).images\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(f\"generated_image_{i}.png\") \n",
    "        print(f\"Generated image {i}:\")\n",
    "        display(image)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display outputs from DreamBooth model\n",
    "\n",
    "To generate different images of the character/data change the prompt ensuring to keep the same phrasing that was used while training. \n",
    "\n",
    "Some additional actions or emotions that were tested during inferencing are: \n",
    "- walking \n",
    "- crying \n",
    "- eating \n",
    "- with hands on face \n",
    "- playing tennis \n",
    "- doing yoga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"An image of UnicornGirl in unicorn onesie running\"\n",
    "num_samples = 5\n",
    "negative_prompt = \"\"\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "inference(prompt, num_samples, negative_prompt, guidance_scale, num_inference_steps, height, width)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
